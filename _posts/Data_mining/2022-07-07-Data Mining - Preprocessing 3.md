---
title : "🧩 Data Mining (10) - Preprocessing_3 : Data Integration - Numerical Data"

categories:
    - Data_mining
tags:
    - [Pandas, Data, Data Mining, Preprocessing, Integration, Variance]

toc : true
toc_sticky : true 
use_math : true  

date: 2022-07-07
last_modified_at: 2022-07-07 
---  

* * *  

🧩 저번 포스팅에서는 범주형 데이터에 대한 data integration 방법인 chi-square test에 대해 알아보았다. 이번에는 <b><a>Numerical Data</a></b>, 즉 수치형 데이터에 대한 방법들을 알아보도록 하자.  

🧩 이 방법들은 아래와 같이 정리할 수 있다.  
    - 분산분석 (Variance)  
    - 공분산 분석 (Covariance)  
    - 상관관계 분석 (Correlation)  
  
👉 이제 variance measure부터 차근차근 알아보도록 하자.  

* * *  

## 1. Variance for single numerical data variable  

🧩 분산, 즉 <a>variance</a>를 다루기 전에 기초적인 통계 관련 지식을 짚고 갈 필요가 있을 듯 하다. 다름 아닌 <a>평균</a> 이다. 각 표본의 합을 표본의 수로 나눈 것을 의미하며, 보통 <a><b>$E(X)$</b></a>, <a><b>$μ$</b></a> 라는 기호를 통해 사용하는 경우가 많다. 사실 거창하게 통계 지식이라고 말은 해뒀지만, 평균만 알면 뒤에 나올 내용들을 이해하는 데에는 전혀 어려움이 없을 것이다. 또한 <a>편차</a>라는 개념을 알아야한다. 편차는 <a>관측값 - 평균</a> 을 의미하며, 주로 <a>$X-μ$</a> 라고 표현된다.  


🧩 위의 두 개념들을 사용하면 분산을 쉽게 설명할 수 있다. 분산은 <a>편차의 제곱의 평균</a>으로 계산되며, 표본이 흩어진 정도를 의미한다. 수식은 아래와 같다.<br>  

<center>$σ^2 = Var(X) = E((X-μ)^2)=E(X^2)-(E(X))^2$</center><br>  

<center>$if\;\,X\;\,is\;\,discrete,\;\;σ^2 =\sum{(X-μ)^2}f(X),\;\;\,and\;\,f(X):$ 확률질량함수</center><br>  


<center>$if\;\,X\;\,is\;\,continuous,\;\;σ^2 =\int_{-\infty}^\infty{(X-μ)^2}f(X),\;\;\,and\;\,f(X):$ 확률밀도함수</center><br>  


👉 위의 두 식에서 볼 수 있듯이 $X$라는 변수가 연속이냐, 불연속이냐에 따라서 정의되는 확률 함수의 형태도 다르고 그 계산 방식도 다르기 때문에 이를 고려해줘야한다. 하지만 이를 데이터마이닝에서 설명하기에는 내용이 통계학쪽에 많이 가깝기 때문에, 나중에 기회가 되면 한번 정리해야겠다.  

🧩 이렇게 분산을 구한 후에는 표준편차, 즉 <a>Standard Deviation</a>를 구한다. 표준편차는 분산에 제곱근을 취한 값을 의미하며, 수식은 아래와 같다.<br>  

<center>$σ = \sqrt{σ^2}$</center><br>  

⭐ 분산과 표준편차를 구함으로써 우리가 알고 싶은 value가 평균 $μ$로부터 얼마나 떨어져있는지를 확인하고, 그 데이터를 선택해도 될지, 통합해도 될지 여부를 알아낼 수 있다.  

* * *  

## 2. Covariance for two variables  

🧩 위에서 살펴본 variance는 단일 variable의 데이터 분포를 알아보는 데에 사용했다면, 이번에 알아볼 <b><a>Covariacne</a></b>, 즉 공분산은 두 variable 사이의 관계를 알아보기 위해 사용한다. 수식은 아래와 같다.<br>  

<center>$σ_{12} = E[(X_1-μ_1)(X_2-μ_2)] = E(X_1X_2)-μ_1μ_2=E(X_1X_2)-E(X_1)E(X_2)$</center><br>  

즉, <center>$σ_{12} = \frac{1}{n}\sum_{i=1}^n(X_{i1}-μ_1)(X_{i2}-μ_2)$</center><br>  

그리고 이때,  

<center>$if\;\;σ_{12}\,>\,0\;:\;positive\;covariance$</center><br>  

<center>$if\;\;σ_{12}\,<\,0\;:\;negative\;covariance$</center><br>  

<center>$if\;\,X_1,\,X2\;\,is\;\,independent\;\,for\;\,each\;\,other,\;\,σ_{12}\,=\,0$</center><br>  

🧩 예시를 한번 살펴보도록 하자.<br>  

ex) $(X_1,X_2) = (2,5)\;(3,8)\;(5,10)\;(4,11)\;(6,14)$<br>  

$μ_1 = (2+3+5+4+6) / 5= 4,\;\;\;\;\;μ_2=(5+8+10+11+14)/5=9.6$<br>  

$σ_{12}=\frac{1}{5}\times{((2-4)(5-9.6)+(3-4)(8-9.6)+(5-4)(10-9.6)+(4-4)(11-9.6)+(6-4)(14-9.6))}=4$<br>  

🧩 $E(X_1X_2)-E(X_1)E(X_2)$ 공식을 사용해서 더 쉽게 구할 수도 있다.<br>  

$σ_{12}=E(X_1X_2)-E(X_1)E(X_2)=\frac{(10+24+50+44+84)}{5}-4\times9.6 = 42.4-38.4=4$<br>  

👉 구한 공분산 $σ_{12}$이 4로 0보다 크기 때문에 두 variable 은 서로 positive한 관계임을 알 수 있다.  

🧩 이렇게 공분산을 통해 두 variable, 즉 attribute들 간의 관계를 알 수 있지만, 이는 단위의 영향을 받는다는 단점을 가지고 있다. 예를 들어 하나의 변수가 cm 단위이고 디른 변수는 m단위라고 가정하면, 공분산은 이를 보정해주는 역할은 해주지 못한다. 이러한 이유로 다른 방법이 필요해졌고, 그렇게 나온 개념이 <a>피어슨 상관계수</a>이다.  

* * *  

## 3. Correlation between two numerical variables  

🧩 상관계수는 수식부터 먼저 보도록 하자.<br>  


<center>$ρ_{12} = \frac{σ_{12}}{σ_1σ_2} = \frac{\sum{(X_{i1}-μ_1)(X_{i2}-μ_2)}}{\sqrt{\sum{(X_{i1}-μ_1)^2(X_{i2}-μ_2)^2}}}$</center><br>  

위의 식만 보면 상당히 복잡해보이지만, 사실은 공분산을 표준편차1과 표준편차2의 곱으로 나눠줌으로써 정규화해주는 것이다. 그리고 이 과정에서 분모와 분자의 단위가 약분이 되어 날아가므로 상관계수는 단위의 영향을 받지 않는다.<br>  

🧩 상관계수의 성질은 아래와 같다.<br>  
    - $\;\;-1\leqqρ_{12}\leqq1$<br>  
    - $\;\;if\;\;ρ_{12}>0\;\,:\;\,positive\;\;correlation$<br>  
    - $\;\;if\;\;ρ_{12}=0\;\,:\;\,independent\;\,for\;\,each\;\,other$<br>  
    - $\;\;if\;\;ρ_{12}<0\;\,:\;\,negative\;\;correlation$<br>  
    - 그리고 상관계수의 크기가 클수록 강한 상관관계가 있음을 의미한다.<br>  

🧩 상관계수 또한 앞에서 다뤘던 다양한 measure들과 같이 이미 만들어진 함수가 있다. 학기 중에 수행한 데이터마이닝 프로젝트에서 해당 함수를 사용한 적이 있기 때문에, 함수의 출력결과를 보는 것으로 이번 포스팅을 마무리하도록 하자.  

<p align="center"><img src="https://user-images.githubusercontent.com/65170165/177690493-b899569e-9474-4994-911b-d88e9b093fab.png" width="600" /></p>  


⭐ <a><b>.corr 함수</b></a> 를 사용해서 각 attribute 간의 상관계수를 계산해주고, 파이썬의 seaborn 라이브러리를 사용하면 시각화까지 이쁘게 해주는 것을 확인 할 수 있다😊😊.  

* * *  

🧩 이렇게 해서 분산과 공분산, 그리고 상관계수에 이르는 내용들을 배웠다. 수식은 복잡하고 간단한 표본이라 해도 계산이 복잡한 경우가 많지만, 위의 함수처럼 편리한 함수가 많이 있기 때문에 얘들을 잘 사용할 줄 아는 것이 더욱 중요할 것이라 생각한다.  

🧩 원래는 데이터마이닝 개념을 먼저 한번씩 살펴보고 프로젝트를 다룰 생각이었는데 이렇게 중간중간에 관련 결과를 넣는 것도 이해에 좋을 것 같다. 앞으로는 최대한 다양한 자료를 사용해서 포스팅을 하도록 노력해봐야겠다.  

🧩 이번 포스팅으로 Data Integration은 한번씩 흝어봤다. 다음 포스팅부터는 Data Reduction에 대해 배워보도록 하자🏃‍♂️🏃‍♂️.  

* * *  

<div style="text-align: left">💡위 포스팅은 한국외국어대학교 바이오메디컬공학부 고윤희 교수님의 [생명정보학을 위한 데이터마이닝] 강의 내용을 바탕으로 함을 밝힙니다.</div>
