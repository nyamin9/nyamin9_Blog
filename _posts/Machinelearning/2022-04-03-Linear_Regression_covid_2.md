---
title : "âš™ 7. [íŒŒì´í† ì¹˜] Covid-19 ë°ì´í„°ë¡œ Linear Regression êµ¬í˜„í•˜ê¸°(2)"

categories:
    - Machinelearning
tags:
    - [AI, Regression, Prediction, Mini_Batch]

toc : true
toc_sticky : true
use_math : true

date: 2022-04-03
last_modified_at: 2022-04-03
---  
* * *

ğŸ”¨ ì•ì„  í¬ìŠ¤íŒ…ì—ì„œ ìƒ˜í”Œë§í•œ train_set(X)ì™€ target(y)ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ í•™ìŠµí•´ë³´ì.  

***
## 1. model, loss, optimization algorithm, data loader ì •ì˜  

```py
### í•„ìš”í•œ í•¨ìˆ˜ë“¤ì˜ ì •ì˜ (model, loss, algorithm, data loader) ###

## 1.Linear Regression model ìƒì„±
# y = XW + b
# retrun predictions(= y_hat)

def lin_model(X, W, b): 
    return torch.matmul(X, W) + b

## 2.loss í•¨ìˆ˜ ìƒì„±
# squared loss : ((predictions - labels)^2) / 2
# yëŠ” torch.Sizeê°€ ([314]), y_hatì€ matmmul ì—°ì‚°ìœ¼ë¡œ ì¸í•´ torch.Sizeê°€ ([314, 1])ì´ë¯€ë¡œ ì—°ì‚°ì„ ìœ„í•´ reshape()í•¨ìˆ˜ ì‚¬ìš©
# yë¥¼ y_hatì˜ shapeìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì—°ì‚°

def loss(y_hat, y):  
    return (y_hat - y.reshape(y_hat.shape))**2 / 2


## 3.optimization algorithm ì •ì˜
# Minibatch stochastic gradient descent
# parameter(W,b) / learning_rate / batch_size ë¥¼ ì¸ìˆ˜ë¡œ ë°›ìŒ
# parameter(W,b) ì—…ë°ì´íŠ¸

def sgd(W, b, lr, batch_size):  
    # with torch.no_grad() êµ¬ë¬¸ ë‚´ì—ì„œëŠ” gradient ì—°ì‚° ì˜µì…˜ì„ êº¼ì¤Œ
    # sgd í•¨ìˆ˜ í˜¸ì¶œ ìˆœê°„ì˜ gradientë¥¼ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ì˜ë¯¸
    with torch.no_grad():
        W -= lr * W.grad / batch_size         # W.grad : Wì˜ gradient 
        W.grad.zero_()                        # Wì˜ gradient 0ìœ¼ë¡œ ì´ˆê¸°í™”
        
        b -= lr * b.grad / batch_size         # b.grad : bì˜ gradient 
        b.grad.zero_()                        # bì˜ gradient 0ìœ¼ë¡œ ì´ˆê¸°í™”
            

## 4.data loader ì •ì˜
# í•™ìŠµë°ì´í„° Xì™€ label y, batch_sizeë¥¼ ì¸ìˆ˜ë¡œ ë°›ìŒ
# í•™ìŠµë°ì´í„°ì˜ ê¸¸ì´(314)ë¥¼ num_examplesë¡œ ë°›ìŒ
# num_exampleì˜ range ë¦¬ìŠ¤íŠ¸(0~313)ë¥¼ indicesë¡œ ë°›ìŒ
# indicesë¥¼ randomìœ¼ë¡œ ì„ìŒ - ë°ì´í„°ì˜ ìˆœì„œê°€ ì„ì„

def data_iter(batch_size, X, y):
    num_examples = len(X)                     
    indices = list(range(num_examples))        
    random.shuffle(indices)                    
    
    # í•™ìŠµì„ ìœ„í•´ ë°ì´í„°ë¥¼ batch_size í¬ê¸°ë¡œ ë‚˜ëˆ ì„œ ë°›ìŒ
    # ì „ì²´ ë°ì´í„°ë¥¼ í•œë°”í€´ ìˆœíšŒí•¨ : 1 epoch
    # batch_indicesì— ëœë¤í•˜ê²Œ ì„ì€ indicesë¥¼ ìŠ¬ë¼ì´ì‹±í•´ì„œ ì¸ë±ìŠ¤ë¥¼ ë°›ì•„ì¤Œ
    # ë¯¼ì•½ i + batch_sizeê°€ ì´ ë°ì´í„°ì˜ ê¸¸ì´ë³´ë‹¤ ê¸¸ì–´ì§€ë©´ ë§ˆì§€ë§‰ ë°ì´í„°ê¹Œì§€ë§Œ ë°›ì•„ì˜´
    # ex) i = 310, batch_size = 10ì´ë©´ ë§ˆì§€ë§‰ì—ëŠ” ì¸ë±ìŠ¤ 310:314 ê¹Œì§€ë§Œ ìŠ¬ë¼ì´ì‹±í•´ì„œ ë°›ìŒ
    # yield ë¡œ batch_sizeë§Œí¼ì˜ Xì™€ yë¥¼ ë°˜í™˜í•´ì¤Œ 
    
    for i in range(0, num_examples, batch_size): 
        batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])
        yield X[batch_indices], y[batch_indices]
```  

ğŸ”¨ data_iter() í•¨ìˆ˜ë¥¼ ì§ì ‘ êµ¬í˜„í•´ì„œ í•™ìŠµì„ ì§„í–‰í•œë‹¤. ë³´í†µì€ íŒŒì´í† ì¹˜ì—ì„œ ì œê³µí•˜ëŠ” <a>Dataset, DataLoader</a> ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ” í¸ì´ì§€ë§Œ, ë°°ìš´ ë‚´ìš©ì„ ë³µìŠµí•´ë³´ê¸° ìœ„í•´ êµ¬í˜„í–ˆë‹¤. ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œ ë‹¤ë£° ìƒê°ì´ë‹¤.  

* * *
## 2. model parameter ì´ˆê¸°í™” / hyperparameter ì„¤ì •
  

ğŸ”¨ ë¨¸ì‹ ëŸ¬ë‹ì— ìˆì–´ì„œ ëª‡ê°€ì§€ ì„¤ì •í•  ë§¤ê°œë³€ìˆ˜ë“¤ì´ ìˆëŠ”ë°, ê° íŠ¹ì§•ì€ ì•„ë˜ì™€ ê°™ë‹¤.  
- parameter : ì‹¤ì œë¡œ ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ì—…ë°ì´íŠ¸ë˜ëŠ” ë³€ìˆ˜
    - <b>W</b> (weight)
    - <b>b</b> (bias)  


- hyperparameter : ì—…ë°ì´íŠ¸ë˜ì§€ëŠ” ì•Šì§€ë§Œ ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ì˜í–¥ì„ ì£¼ëŠ” ë³€ìˆ˜
    - <b>lr</b> (learning rate, í•™ìŠµë¥ )
    - <b>num_epochs</b> (epoch, í•™ìŠµì‹œí–‰íšŸìˆ˜)
    - <b>batch_size</b> (ë°°ì¹˜ì‚¬ì´ì¦ˆ)  

```py
### model parameter ì´ˆê¸°í™” ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ###

## parameter ì´ˆê¸°í™”
# ê° parameter ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ requires_grad = True ì„¤ì •
# requires_grad = True ì„¤ì •ìœ¼ë¡œ W.gradì™€ b.gradì— ê° parameterì˜ gradientê°€ ì €ì¥ë˜ì–´ gradientë¡œ ì‚¬ìš©ê°€ëŠ¥í•¨
# Xì™€ì˜ matrix multiplication ì„ ìœ„í•´ Xì˜ ì—´ì˜ ê°œìˆ˜ì™€ Wì˜ í–‰ì˜ ê°œìˆ˜ë¥¼ ë§ì¶°ì¤Œ

W = torch.zeros((8,1), requires_grad = True)        # Weight ì´ˆê¸°ê°’ 0ìœ¼ë¡œ ì„¤ì •
b = torch.zeros(1, requires_grad = True)            # bias ì´ˆê¸°ê°’ 0ìœ¼ë¡œ ì„¤ì •


## hyper parameter ì„¤ì •

lr = 0.0001                  # learning rate ì„¤ì •
num_epochs = 100000          # ì‹œí–‰íšŸìˆ˜ ì„¤ì •
batch_size = 50              # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
```  

* * *  
## 3. í•™ìŠµ - Minibatch Stochastic Gradient Descent
  
ğŸ”¨ ë¨¸ì‹ ëŸ¬ë‹ ê³µë¶€ë¥¼ í•˜ë‹¤ ë³´ë©´ ê¼­ ë§Œë‚˜ëŠ” ë‚´ìš©ì¸ <a>gradient descent</a> ì˜ ì¢…ë¥˜ë“¤ì´ë‹¤. ì‰¬ìš´ ì´í•´ë¥¼ ìœ„í•´ì„œ í•™ìŠµ ì‹œê°„ì„ ì„ì˜ë¡œ ì„¤ì •í–ˆë‹¤ğŸ™„.  

- 1. <b>Gradient Descent</b> (,Batch Gradient Descent)
    - <a>ë°ì´í„°ë¥¼ í•œ ë°”í€´ ë‹¤ ëŒë©´ í•œë²ˆ ì—…ë°ì´íŠ¸</a>
    - ì—…ë°ì´íŠ¸ í•œë²ˆì— ê±¸ë¦¬ëŠ” ì‹œê°„ : 1ì‹œê°„  

- 2. <b>Stochastic Gradient Descent</b>
    - <a>ë°ì´í„°ì˜ ê° feature í•˜ë‚˜ ëŒ ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸</a>
    - ì—…ë°ì´íŠ¸ í•œë²ˆì— ê±¸ë¦¬ëŠ” ì‹œê°„ : 1ì´ˆ  

- 3. <b>Minibatch Stochastic Gradient Descent</b>
    - <a>ì„¤ì •í•œ batch_sizeë¥¼ ëŒë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸</a>
    - ì—…ë°ì´íŠ¸ í•œë²ˆì— ê±¸ë¦¬ëŠ” ì‹œê°„ : 1ë¶„  

```py
### for loopë¥¼ ì´ìš©í•œ modelì˜ í•™ìŠµ ###

# ì´ ì‹œí–‰íšŸìˆ˜ num_epochs ë™ì•ˆ forë¬¸ ì‹¤í–‰
# data_iter í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ X[batch_indices], y[batch_indices]ë¥¼ ê°ê° batch_features, batch_labelsìœ¼ë¡œ ë°›ì•„ì„œ squared loss ì—°ì‚° ì§„í–‰
for epoch in range(num_epochs):
    for batch_features, batch_labels in data_iter(batch_size, X, y):
        l = loss(lin_model(batch_features, W, b), batch_labels)  
        
        # Compute gradient on `l` with respect to [`w`, `b`]
        #.backward()ë¥¼ í˜¸ì¶œí•˜ì—¬ lossì— ëŒ€í•œ Wì™€ bì˜ gradientë¥¼ ê³„ì‚°í•˜ê³  ì €ì¥í•¨
        l.sum().backward()
        
        # sgdí•¨ìˆ˜ - parameter(W,b) ì—…ë°ì´íŠ¸
        sgd(W, b, lr, batch_size)
        
    # with torch.no_grad() êµ¬ë¬¸ ë‚´ì—ì„œëŠ” í•™ìŠµê°€ëŠ¥í•œ parameterë“¤ì„ ì—…ë°ì´íŠ¸ í•˜ì§€ ì•ŠìŒ
    # W, bì˜ ì—…ë°ì´íŠ¸ ì—†ìŒ
    # train_lì— lossê°’ì„ ë°›ì•„ì„œ epoch 1000ë²ˆë‹¹ í•œë²ˆì”© lossì˜ í‰ê· ì„ ì¶œë ¥í•¨
    with torch.no_grad():                              
        train_l = loss(lin_model(X, W, b), y)  

        if epoch%1000==0:
            print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```
```
>>
epoch 1, loss 38003.773438
epoch 1001, loss 19245.431641
epoch 2001, loss 18821.029297
epoch 3001, loss 18648.062500
epoch 4001, loss 18526.097656
epoch 5001, loss 18416.160156
epoch 6001, loss 18331.574219
epoch 7001, loss 18309.513672
epoch 8001, loss 18262.779297
epoch 9001, loss 18163.675781

. . .

epoch 90001, loss 17740.068359
epoch 91001, loss 17673.435547
epoch 92001, loss 17668.839844
epoch 93001, loss 17686.435547
epoch 94001, loss 17681.234375
epoch 95001, loss 17723.623047
epoch 96001, loss 17675.238281
epoch 97001, loss 17684.761719
epoch 98001, loss 17671.396484
epoch 99001, loss 17670.376953
```  
* * *
## 4. ê²°ê³¼

ğŸ”¨ í•™ìŠµê²°ê³¼ë¥¼ ì•„ë˜ì™€ ê°™ì´ ìš”ì•½í•˜ì.
- 314ê°œ exampleë“¤ì— ëŒ€í•œ lossì˜ í‰ê· 
- 314ê°œ exampleì— ëŒ€í•œ label, predictionì˜ scatter plot
- í•™ìŠµì´ ì™„ë£Œëœ modelì˜ weightë“¤ì˜ ê°’  

```py
# with torch.no_grad() - requires_grad = True ì¸ Tensor(weight, bias)ì˜ ì—…ë°ì´íŠ¸ë¥¼ ë©ˆì¶¤
# linear regression modelì˜ ê²°ê³¼ê°’ì„ y_hat(prediction) ìœ¼ë¡œ ë°›ìŒ
# train_lì— lossë¥¼ ë°›ì•„ì„œ lossì˜ í‰ê· ì„ ì¶œë ¥í•¨  
with torch.no_grad():
    y_hat = lin_model(X, W, b)
    train_l = loss(y_hat, y)
    print(f'average loss: {float(train_l.mean()):f}')

# scatter plot ìƒì„±
# xì¶•ì— ì‹¤ì œ label(y) ê°’, yì¶•ì— prediction(y_hat) ê°’ì„ ì°ì–´ì¤Œ. ì ì˜ sizeëŠ” 2ë¡œ ì„¤ì •
# xì¶•ê³¼ yì¶•ì— np.arange()í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ 0~1200 êµ¬ê°„ì˜ ë°°ì—´ì„ ê°„ê²© 10ì„ ê¸°ì¤€ìœ¼ë¡œ ì°ì–´ì¤Œ. ì ì˜ sizeëŠ” 1ë¡œ ì„¤ì •
# .grid('on') ì˜µì…˜ìœ¼ë¡œ ë°°ê²½ì— ê²©ìë¬´ëŠ¬ ìƒì„±
plt.figure(dpi=100)
plt.scatter(y,y_hat,2)
plt.scatter(np.arange(0,1200,10),np.arange(0,1200,10),1)
plt.grid('on')

# xì¶• ì´ë¦„, yì¶• ì´ë¦„, scatter plotì˜ ì´ë¦„ ì„¤ì •
plt.xlabel('real value (y)')
plt.ylabel('prediction (y_hat)')
plt.title(f'final loss {float(train_l.mean()):f}')
plt.show()

# ê° weightë“¤ì˜ ê°’ê³¼ bias ê°’ ì¶œë ¥
# .dethach()ë¥¼ ì‚¬ìš©í•´ ê¸°ì¡´ Tensorì—ì„œ gradient ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ” í…ì„œë¥¼ ìƒì„±í•˜ì—¬ ì¶œë ¥
print('W:',W.detach())
print('b:',b.detach())
```
```
>> average loss: 17668.511719
```

<p align="center"><img src="https://user-images.githubusercontent.com/65170165/161427606-f25a81b6-d127-43cb-b61e-58ee4728f3d8.png" width="600" /></p>  

```
>>
W: tensor([[ 21.0286],
        [ -5.6660],
        [ 11.9505],
        [-26.1676],
        [  0.5967],
        [ -0.3037],
        [ 10.0532],
        [ 48.4699]])
b: tensor([177.4695])
```  

ğŸ”¨ ë§ˆì§€ë§‰ scatter plotì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ì‹¤ì¸¡ê°’ê³¼ ì˜ˆì¸¡ê°’ ì‚¬ì´ì˜ ì˜ˆì¸¡ë¥ ì´ ë§ì´ ë–¨ì–´ì§€ëŠ” í¸ì´ê³ , ê·¸ì— ë”°ë¼ ì„ í˜•ê´€ê³„ë„ ê±°ì˜ ì—†ìŒì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤ğŸ¤¦â€â™‚ï¸. ì• ì´ˆì— ë°ì´í„° ì—­ì‹œ linearí•˜ì§€ ì•Šì•˜ê³ , ì¶©ë¶„í•œ í•™ìŠµì„ ì‹œí‚¤ê¸°ì—ëŠ” train_setì˜ ê°œìˆ˜ë„ ë§ì€ í¸ì´ ì•„ë‹ˆë¼ì„œ ìœ„ì™€ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ ê²ƒ ê°™ë‹¤.  

* * *
ğŸ”¨ ì´ë ‡ê²Œ ë°ì´í„°ë¥¼ train_setê³¼ targetìœ¼ë¡œ ë‚˜ëˆ„ê³  í•™ìŠµí•˜ëŠ” ì¼ë ¨ì˜ ê³¼ì •ì„ ì €ë²ˆ í¬ìŠ¤íŒ…ê³¼ ì´ë²ˆ í¬ìŠ¤íŒ…ì„ í†µí•´ ë‹¤ë¤˜ë‹¤. ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œëŠ” íŒŒì´í† ì¹˜ì˜ ëª¨ë“ˆì„ ê°€ì§€ê³  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ë²•ì„ ì•Œì•„ë³´ìğŸ˜‰. 
